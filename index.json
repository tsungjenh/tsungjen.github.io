[{"categories":["Go"],"contents":" Intro There are two classic methods in Go to handle concurrency. One is WaitGroup, the another one is Context. In this article, we are going to talk about Context and explore the source code to see how it works behind the scenes. After the source code review, we will gain more insight of how to use Context properly.\n source code: src/context/context.go   Interface First, Take a look at the interface\ntype Context interface { Deadline() (deadline time.Time, ok bool) Done() \u0026lt;-chan struct{} Err() error Value(key interface{}) interface{} } There are 4 methods for this interface.\n Deadline: Deadline is a method to get the deadline of the context. When deadline is due, \u0008the context will raise a cancellation automatically. The second return value ok is to indicate whether the context has a deadline or not. If ok equals false, there is no deadline for the context Done: Done is a method to get a read-only channel. We can read the channel and wait for its parent context to raise the cancellation Err: Err returns the reason to cancel Value: To get the value binded to the context. (We can bind the value to the context with a key)  There is a basic implimentation, emptyCtx, for Context and 2 objects are initiated to use, Background and TODO.\n// An emptyCtx is never canceled, has no values, and has no deadline. It is not // struct{}, since vars of this type must have distinct addresses. type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() \u0026lt;-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } func (e *emptyCtx) String() string { switch e { case background: return \u0026#34;context.Background\u0026#34; case todo: return \u0026#34;context.TODO\u0026#34; } return \u0026#34;unknown empty Context\u0026#34; } var ( background = new(emptyCtx) todo = new(emptyCtx) ) // Background returns a non-nil, empty Context. It is never canceled, has no // values, and has no deadline. It is typically used by the main function, // initialization, and tests, and as the top-level Context for incoming // requests. func Background() Context { return background } // TODO returns a non-nil, empty Context. Code should use context.TODO when // it\u0026#39;s unclear which Context to use or it is not yet available (because the // surrounding function has not yet been extended to accept a Context // parameter). func TODO() Context { return todo } We can see emptyCtx do nothing. All the methods simply return nil. We can not set the deadline, cancel or bind the value to the Context.\nTODO is temporary used when we are not sure which Context implimentation to use.\nBackground is a used as a root Context and to create the child Context with following 4 functions.\nfunc WithCancel(parent Context) (ctx Context, cancel CancelFunc) func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) func WithValue(parent Context, key, val interface{}) Context Let\u0026rsquo;s go through the functions one by one.\n WithCacnel func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } c := newCancelCtx(parent) propagateCancel(parent, \u0026amp;c) return \u0026amp;c, func() { c.cancel(true, Canceled) } } // newCancelCtx returns an initialized cancelCtx. func newCancelCtx(parent Context) cancelCtx { return cancelCtx{Context: parent} } // A cancelCtx can be canceled. When canceled, it also cancels any children // that implement canceler. type cancelCtx struct { Context mu sync.Mutex // protects following fields \tdone chan struct{} // created lazily, closed by first cancel call \tchildren map[canceler]struct{} // set to nil by the first cancel call \terr error // set to non-nil by the first cancel call } WithCancel will receive a parent context and return a cancelCtx object along with a cancel function. In the function we will\n Create a cancelCtx Call propagateCancel to arrange this context to be canceled when parent is canceled.  A cancelCtx has a\n Context to point to its parent mutex to ensure thread-safe property (lock the context when we r/w the context), done to push the cancellation signal children to store its future children (any context can be a parent context) err to store the reason for cancellation.  func (c *cancelCtx) Value(key interface{}) interface{} { if key == \u0026amp;cancelCtxKey { return c } return c.Context.Value(key) } func (c *cancelCtx) Done() \u0026lt;-chan struct{} { c.mu.Lock() if c.done == nil { c.done = make(chan struct{}) } d := c.done c.mu.Unlock() return d } func (c *cancelCtx) Err() error { c.mu.Lock() err := c.err c.mu.Unlock() return err } The implementation is quite simple and the lock makes it thread-safe\nAfter a look to cancelCtx, Take a look at the propagateCancel function.\nfunc propagateCancel(parent Context, child canceler) { done := parent.Done() if done == nil { return // parent is never canceled \t} select { case \u0026lt;-done: // parent is already canceled \tchild.cancel(false, parent.Err()) return default: } if p, ok := parentCancelCtx(parent); ok { p.mu.Lock() if p.err != nil { // parent has already been canceled \tchild.cancel(false, p.err) } else { if p.children == nil { p.children = make(map[canceler]struct{}) } p.children[child] = struct{}{} } p.mu.Unlock() } else { atomic.AddInt32(\u0026amp;goroutines, +1) go func() { select { case \u0026lt;-parent.Done(): child.cancel(false, parent.Err()) case \u0026lt;-child.Done(): } }() } } What propagateCancel does is to\n Check parent is already canceled (parent can be canceled, meanwhile, we call WithCancel function). If its parent is canceled, cancel itself as well. If parent is not canceled. Check if parent is cancelCtx, if yes add itself as its parent\u0026rsquo;s child. Otherwise, start a go routine to listen to a done signal and see if its parent is done (the goroutine will trigger the cacnellation when parent is done).   WithDeadline func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } if cur, ok := parent.Deadline(); ok \u0026amp;\u0026amp; cur.Before(d) { // The current deadline is already sooner than the new one. \treturn WithCancel(parent) } c := \u0026amp;timerCtx{ cancelCtx: newCancelCtx(parent), deadline: d, } propagateCancel(parent, c) dur := time.Until(d) if dur \u0026lt;= 0 { c.cancel(true, DeadlineExceeded) // deadline has already passed \treturn c, func() { c.cancel(false, Canceled) } } c.mu.Lock() defer c.mu.Unlock() if c.err == nil { c.timer = time.AfterFunc(dur, func() { c.cancel(true, DeadlineExceeded) }) } return c, func() { c.cancel(true, Canceled) } } WithDeadline returns a copy of the parent context with the deadline adjusted to be no later than d. If the parent\u0026rsquo;s deadline is already earlier than d, WithDeadline(parent, d) is semantically equivalent to parent. The returned context\u0026rsquo;s Done channel is closed when the deadline expires, when the returned cancel function is called, or when the parent context\u0026rsquo;s Done channel is closed, whichever happens first.\nTake a look at timerCtx\ntype timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu.  deadline time.Time } func (c *timerCtx) Deadline() (deadline time.Time, ok bool) { return c.deadline, true } func (c *timerCtx) String() string { return contextName(c.cancelCtx.Context) + \u0026#34;.WithDeadline(\u0026#34; + c.deadline.String() + \u0026#34; [\u0026#34; + time.Until(c.deadline).String() + \u0026#34;])\u0026#34; } func (c *timerCtx) cancel(removeFromParent bool, err error) { c.cancelCtx.cancel(false, err) if removeFromParent { // Remove this timerCtx from its parent cancelCtx\u0026#39;s children. \tremoveChild(c.cancelCtx.Context, c) } c.mu.Lock() if c.timer != nil { c.timer.Stop() c.timer = nil } c.mu.Unlock() } timerCtx is built on cancelCtx, it has\n cancelCtx created with its parent. It takes care of the cancellation process deadline to store a deadline, we can get it by calling Deadline method   The cancellation is quite the same as cancelCtx except it needs to stop the timer when cancellation is triggered\n  WithTimeout func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } WithTimeout is a the same as WithDeadline. It takes a time.Duration as an input, the deadline will be now + duration.\n WithValue // WithValue returns a copy of parent in which the value associated with key is // val. // // Use context Values only for request-scoped data that transits processes and // APIs, not for passing optional parameters to functions. // // The provided key must be comparable and should not be of type // string or any other built-in type to avoid collisions between // packages using context. Users of WithValue should define their own // types for keys. To avoid allocating when assigning to an // interface{}, context keys often have concrete type // struct{}. Alternatively, exported context key variables\u0026#39; static // type should be a pointer or interface. func WithValue(parent Context, key, val interface{}) Context { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } if key == nil { panic(\u0026#34;nil key\u0026#34;) } if !reflectlite.TypeOf(key).Comparable() { panic(\u0026#34;key is not comparable\u0026#34;) } return \u0026amp;valueCtx{parent, key, val} } // A valueCtx carries a key-value pair. It implements Value for that key and // delegates all other calls to the embedded Context. type valueCtx struct { Context key, val interface{} } // stringify tries a bit to stringify v, without using fmt, since we don\u0026#39;t // want context depending on the unicode tables. This is only used by // *valueCtx.String(). func stringify(v interface{}) string { switch s := v.(type) { case stringer: return s.String() case string: return s } return \u0026#34;\u0026lt;not Stringer\u0026gt;\u0026#34; } func (c *valueCtx) String() string { return contextName(c.Context) + \u0026#34;.WithValue(type \u0026#34; + reflectlite.TypeOf(c.key).String() + \u0026#34;, val \u0026#34; + stringify(c.val) + \u0026#34;)\u0026#34; } func (c *valueCtx) Value(key interface{}) interface{} { if c.key == key { return c.val } return c.Context.Value(key) } WithValue returns a copy of parent in which the value associated with key is val\nvalueCtx has a\n context to point to its parent key, value to store a k-v pair  The lookup for the value is a recursive process. A goroutine will recursively find the key, if the key is not existed in all the nodes, the root (Background) will return a nil (emptyCtx implementation).\n   Noted that valueCtx doesn\u0026rsquo;t ensure the val is thread-safe.\n  Conclusion After we explore the source code of Context, we have conclusion as belows\n Do not put Context inside the structure. Pass it as a parameter Make the Context the first paramter in a function Do not pass nil when you passing Context. It will cause panic if someone uses nil to create child Context. Pass context.TODO if you dont know what to pass. Use valueCtx properly, pass the required data only (such as request meta data). valueCtx doesn\u0026rsquo;t ensure thread-safe for value, passing unnecessary data may cause dirty data issues. Context itself is thread-safe, feel free pass it to different goroutines.  ","permalink":"https://tsungjenh.github.io/blog/go_context/","tags":["Go","Scheduler"],"title":"Go Context Source Code review"},{"categories":["Go"],"contents":" 1. Intro Map is an important data structure in Go. As a programmer, I use it in programming tasks everyday. However, I have very little knowledge on the implementation detail of map which may cause dagerous and unexpected bugs. In this article, I want to deeply look inside the source code and understand the implemntation of Map.\n source code: src/runtime/map.go\n Data Structure First let take look at the data structure.\n// A header for a Go map. type hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/gc/reflect.go. \t// Make sure this stays in sync with the compiler\u0026#39;s definition. \tcount int // # live cells == size of map. Must be first (used by len() builtin) \tflags uint8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) \tnoverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details \thash0 uint32 // hash seed  buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. \toldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing \tnevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated)  extra *mapextra // optional fields }    Name Definition     count is to indicate the size of the map(used by len() builtin)   flags represents map status   B a map has 2^B buckets   noverflow number of overflow buckets   hash0 a hash seed, a random number   buckets points to array of 2^B Buckets. may be nil if count==0   oldbuckets points to the old buckets. If it points to nil, the map is not growing    const ( // Maximum number of key/elem pairs a bucket can hold.  bucketCntBits = 3 bucketCnt = 1 \u0026lt;\u0026lt; bucketCntBits // ... ) // A bucket for a Go map. type bmap struct { // tophash generally contains the top byte of the hash value \t// for each key in this bucket. If tophash[0] \u0026lt; minTopHash, \t// tophash[0] is a bucket evacuation state instead. \ttophash [bucketCnt]uint8 // Followed by bucketCnt keys and then bucketCnt elems. \t// NOTE: packing all the keys together and then all the elems together makes the \t// code a bit more complicated than alternating key/elem/key/elem/... but it allows \t// us to eliminate padding which would be needed for, e.g., map[int64]int8. \t// Followed by an overflow pointer. } bmap is the bucket for Go map, it can store 8 pairs of k-v by default. It store the actual key/value data in map.\n The actual key/value and overflow is added in the compile time.\n A visualisation of Go map is as belows.\n   2. Operations Map in Go is implementation of a hashmap. There are 5 main operations, including creation, get, assign, deletion, growth and iteration. Let\u0026rsquo;s go through the implementation and detail one by one\nCreation func makemap(t *maptype, hint int, h *hmap) *hmap { mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem \u0026gt; maxAlloc { hint = 0 } // initialize Hmap \tif h == nil { h = new(hmap) } h.hash0 = fastrand() // Find the size parameter B which will hold the requested # of elements. \t// For hint \u0026lt; 0 overLoadFactor returns false since hint \u0026lt; bucketCnt. \tB := uint8(0) for overLoadFactor(hint, B) { B++ } h.B = B // allocate initial hash table \t// if B == 0, the buckets field is allocated lazily later (in mapassign) \t// If hint is large zeroing this memory could take a while. \tif h.B != 0 { var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil { h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow } } return h } The map creation is quite straight forward. A random number for hash0 and caculate a appropriate size for B and create the buckets. The bucket size B is caculated by a overload factor which we will discuss later.\nGet func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if raceenabled \u0026amp;\u0026amp; h != nil { callerpc := getcallerpc() pc := funcPC(mapaccess1) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026amp;\u0026amp; h != nil { msanread(key, t.key.size) } if h == nil || h.count == 0 { if t.hashMightPanic() { t.hasher(key, 0) // see issue 23734 \t} return unsafe.Pointer(\u0026amp;zeroVal[0]) } if h.flags\u0026amp;hashWriting != 0 { throw(\u0026#34;concurrent map read and map write\u0026#34;) } hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash\u0026amp;m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { // There used to be half as many buckets; mask down one more power of two. \tm \u0026gt;\u0026gt;= 1 } oldb := (*bmap)(add(c, (hash\u0026amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) { b = oldb } } top := tophash(hash) bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } if t.key.equal(key, k) { e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { e = *((*unsafe.Pointer)(e)) } return e } } } return unsafe.Pointer(\u0026amp;zeroVal[0]) } In the access function, First it checks the flag status to see if there is any goroutine writing to the map. If there is any go routine writing to the map, a panic occurs. This means map in Go is not thread-safe. Next, it get the hash value(uint64) from the key, noted that different maptype has its own hash and equal methods. It will use the hash value to to further find the actual location of the key in the map.\nIn the implementation we can see that the last B bits is used to find the nth buckets in the bucket list and it will loop over the bucket and see if the key match first 8 bits(generate by tophash(hash)) of the hash value. The finding process is a 2 layers for-loop. It will try to match the hash with in the bucket. If no key is match, it will continue to match key in the bucket in the overflow bucket.\n  Assign // Like mapaccess, but allocates a slot for the key if it is not present in the map. func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if h == nil { panic(plainError(\u0026#34;assignment to entry in nil map\u0026#34;)) } // ... \tif h.flags\u0026amp;hashWriting != 0 { throw(\u0026#34;concurrent map writes\u0026#34;) } hash := t.hasher(key, uintptr(h.hash0)) // Set hashWriting after calling t.hasher, since t.hasher may panic, \t// in which case we have not actually done a write. \th.flags ^= hashWriting if h.buckets == nil { h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) \t} again: bucket := hash \u0026amp; bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointer bucketloop: for { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if isEmpty(b.tophash[i]) \u0026amp;\u0026amp; inserti == nil { inserti = \u0026amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) } if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } if !t.key.equal(key, k) { continue } // already have a mapping for key. Update it. \tif t.needkeyupdate() { typedmemmove(t.key, k, key) } elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) goto done } ovf := b.overflow(t) if ovf == nil { break } b = ovf } // Did not find mapping for key. Allocate new cell \u0026amp; add entry.  // If we hit the max load factor or we have too many overflow buckets, \t// and we\u0026#39;re not already in the middle of growing, start growing. \tif !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again \t} if inserti == nil { // all current buckets are full, allocate a new one. \tnewb := h.newoverflow(t, b) inserti = \u0026amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) } // store new key/elem at insert position \tif t.indirectkey() { kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem } if t.indirectelem() { vmem := newobject(t.elem) *(*unsafe.Pointer)(elem) = vmem } typedmemmove(t.key, insertk, key) *inserti = top h.count++ done: if h.flags\u0026amp;hashWriting == 0 { throw(\u0026#34;concurrent map writes\u0026#34;) } h.flags \u0026amp;^= hashWriting if t.indirectelem() { elem = *((*unsafe.Pointer)(elem)) } return elem } In the assign process, first, it checks if the map is empty or there is any goroutine writing to the map. For either cases, a panic occurs. Next, it updates the flag to writing status. There are 2 cases for the assignment - key update or key insertion. A goroutine will try to find the key in the map first and update it, if the key is not found it will do the insert operation. Key update is simple because the map doesn\u0026rsquo;t increase the size, hence, no map growth is in consideration.\nFor case of key insertion, a goroutine will do the map growth by checking\nif !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } After the growth is done, it will insert the key/value to the inserti position which is reserver during the for-loop. The writing flag will be switched off in the end.\nDelete The delete process is similar to the get except the writing flag will be switched on during the deletion. The process is simple, get the position and remove the key, value.\nExpand func hashGrow(t *maptype, h *hmap) { // If we\u0026#39;ve hit the load factor, get bigger. \t// Otherwise, there are too many overflow buckets, \t// so keep the same number of buckets and \u0026#34;grow\u0026#34; laterally. \tbigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { bigger = 0 h.flags |= sameSizeGrow } oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) flags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { flags |= oldIterator } // commit the grow (atomic wrt gc) \th.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 if h.extra != nil \u0026amp;\u0026amp; h.extra.overflow != nil { // Promote current overflow buckets to the old generation. \tif h.extra.oldoverflow != nil { throw(\u0026#34;oldoverflow is not nil\u0026#34;) } h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil } if nextOverflow != nil { if h.extra == nil { h.extra = new(mapextra) } h.extra.nextOverflow = nextOverflow } // the actual copying of the hash table data is done incrementally \t// by growWork() and evacuate(). }  3. Conclusion Go map is a classical hash map implementation. It deals with a set of common problem in hashmap implementation including the bucket data structure, hash function, growth strategies design etc\u0026hellip; After go through the source code, we found the implementation is quite straight forward and easy to understand. We also found that with using the map, we need to always be care of the thread-safe issues. Go map read/write is not thread-safe, prevent multiple go routines write to the same map or use a lock properly.\n","permalink":"https://tsungjenh.github.io/blog/go_map/","tags":["Go","Map"],"title":"Go Map Source Code Review"},{"categories":["Go"],"contents":" Intro One of the biggest feature for Go is the Go Scheduler. The scheduler allows goroutines sheduling works in the user space and has very good performance for concurrency. In this article, I want to talk about what is go scheduler and how does it work. By understanding go scheduler will allow you to make good decision and use the goroutines correctly.\n Why do we need scheduler in Go? A short answer: Becaues Go uses goroutines. Goroutines are user space threads that run uppon the OS thread. The operating system is not aware of goroutines since it run in the user space. Thus, Go needs a scheduler in runtime to handle and schedule the state of goroutines. In other words, Go scheduler multiplexes goroutines onto kernel threads.\n Reference for the reasons why Go use goroutines. Goroutine v.s. Thread\n  When do it schedules? There are 4 classical scenes that Go scheduler needs to make scheduling decision.\n Creation of goroutines: either schedule to run or put it into the wait queue System calls: switch the goroutine and allow other goroutine to use the CPU I/O Blocking: switch the goroutine and allow other goroutine to use the CPU I/O Complete: put the blocked goroutine back to the wait queue  From the example, you can see the goal of the schedule is to optimize the usage of the CPU and try to have the CPU always running if there is any goroutine is waiting for the computing resource.\n For (2), (3), a kernel thread may be blocked.\n  Is Go scheduler preemptive? Based on Go 1.14, Go scheduler is preemptive. The Go 1.14 introduces a new technique of asynchronous preemption which is triggered based on a time condition. There is a thread sysmon, dedicated to watching long-running goroutines and emit a signal if a long-running goroutine is found. Once a signal is received, Go will push an instruction to the program counter, to make it looks like the running program called a function in the runtime. This function parks the goroutine and hands it to the scheduler that will run another one.\n Noted that a goroutine cannot be blocked in anywhere. The current instruction should be a safe point.\n  The \u0026ldquo;MPG\u0026rdquo; Model   There are 3 main components in Go scheduler.\n M(Machine): Kernel Thread. This is the thread managed by the OS P(Processor): A logical processor to manage the Local Run Queue(LRQ) and put the goroutine to run in the \u0026ldquo;M\u0026rdquo; G(Goroutine): The goroutines  For a N cores CPU machines, the maximal parallels we can have is equal to N which means that we will have at most N MPG pairs in the system. A visualization of a 2 cores machine is as belows.\n  There are 2 types of wait queue in the model. Local Run Queue and Global Run Queue.\n Local run queue has higher priority to be execute. Access by the process only. Unless there is a stealing happaned Global run queue has lower priority. Once the local run queue is empty. Processor will consider to fetch goroutine from the Global Run Queue  The idea of local run queue is to prevent the resoucre competition from each processor. If we only have one global queue, everytime a processor access a queue needs to acquire the lock(prevent race condition) which will cause a bad performance.\n Context Switch The context switch for goroutines is in the thread and when a switch occurs, only 3 registers need to be saved/restored - Program Counter, Stack Pointer, and DX. From the OS\u0026rsquo;s perspective Go program behaves as an event-driven program. A waiting goroutine will be placed in the wait queue for later execution. Therefore even thousands of goroutine will not effect the context switch since processor fetch from a queue with O(1) complexity\n When a thread is idle(local queue is empty) To optimize the CPU usage, Go scheduler will prevent the M(Thread) to be idle. In others words, always have goroutines in each local run queue if possible. If the local run queue is empty, the strategy is as belows.\nruntime.schedule() { // only 1/61 of the time, check the global runnable queue for a G.  // if not found, check the local queue.  // if not found,  // try to steal from other Ps.  // if not, check the global runnable queue. }  1/61 of the time, fetch a goroutine from global run queue. Try to steal from other MPG\u0026rsquo;s Local run queue  60/61 of the times, Go scheduler will do a work stealing. It will randomly pick a process a steal a half of the goroutines from its local runqueue. If it fail to steal, then it will try to fetch from the global queue instead. Again, we always try to avoid access global queue which requires the lock acquisition.\n When a thread is blocked by OS If the kernel thread is blocked (ex: syscall), the M(machine) attached to it will also be blocked. There may be a list of goroutines in the local run queue waiting for the execution. If we don\u0026rsquo;t handle it, the goroutines in the local run queue will wait until the thread is released. A straight forward solution is to put those goroutines into the global run queues. However, accessing global run queue requires a lock acquisition which causes a bad performance. In fact, Go scheduler instead of put it into the global run queue, it will try to find an idle thread to take over the \u0026ldquo;P\u0026rdquo; and its LRQ. After the OS Thread is released, Go scheduler will try to see there is any \u0026ldquo;P\u0026rdquo; with LRQ is looking for the idle thread. If it can\u0026rsquo;t find it, It will put the \u0026ldquo;G\u0026rdquo; to the global run queue then go to sleep.\n   Spinning Thread You may feel weird why is there any idle thread. Isn\u0026rsquo;t Go scheduler suppose to prevent threads from being idle? Before we answer the question, we must know an OS Thread is much more heavier than a goroutine. To frequently create or destroy a OS Thread might hurt the performance. In the case of thread block (the P and LRQ is missing the \u0026ldquo;M\u0026rdquo;(Thread)), We can either wait until the block is released or find a new thread to take over the P and LRQ. Instead of creating the thread at the moment, Go introduces spinning thread to have thread standby to serve. It may hurt extra CPU power, but it will minimize the cost if a preemption or a block occur. A thread is spinning if:\n An \u0026ldquo;M\u0026rdquo; with a \u0026ldquo;P\u0026rdquo; assignment is looking for a runnable goroutine. An \u0026ldquo;M\u0026rdquo; without a \u0026ldquo;P\u0026rdquo; assignment is looking for available \u0026ldquo;Ps\u0026rdquo;. Scheduler also unparks an additional thread and spins it when it is readying a goroutine if there is an idle \u0026ldquo;P\u0026rdquo; and there are no other spinning threads. There are at most GOMAXPROCS spinning \u0026ldquo;Ms\u0026rdquo; at any time. When a spinning thread finds work, it takes itself out of spinning state.  Idle threads with a \u0026ldquo;P\u0026rdquo; assignment don’t block if there are idle Ms without a \u0026ldquo;P\u0026rdquo; assignment. When new goroutines are created or an \u0026ldquo;M\u0026rdquo; is being blocked, scheduler ensures that there is at least one spinning \u0026ldquo;M\u0026rdquo;. This ensures that there are no runnable goroutines that can be otherwise running; and avoids excessive M blocking/unblocking.\n","permalink":"https://tsungjenh.github.io/blog/go_scheduler/","tags":["Go","Scheduler"],"title":"Understanding Go Scheduler"},{"categories":["Go"],"contents":" Introduction A Garbage collector is a design of automatic heap memory management, it attempts to free up the allocations that are no long needed while keeping the allocations that are still in use. It makes a programmer\u0026rsquo;s life easier, with GC, it helps to prevent bugs such as dangling pointer bugs, double free bugs, memory leaks etc. On the other hand, it consumes more CPU and introduces more latency.\nIn this article, we are going to talk about the process of garbage collection in Go, including the phase of GC, strategies and design. By understanding GC, a developer can make better decision on memory allocation and write better code.\n Mark-Sweap Mark-Sweap is a classical algorithms to implement a garbage collector. The process including a mark phase and a sweep phase.\n Mark Phase - Mark the survival objects starting from root objects    2. Sweep Phase - Loop over the objects and free up the unmarked objects\n   The garbage collection in Go is based on the idea of Mark-Sweep.\n  Stategies 1. Stop The World (STW) A goroutine can update the object reference during the garbage collection. This can lead to a free up to a in used object. To prevent this case, we need to stop all the running goroutine(Stop the world) during GC. However, STW means all the goroutines are not working which will lead to a bad performance.   2. tri-color marking In Go garbage collection, it uses tri-color marking algorithms. The algorithms marks all the objects into 3 colors - white, gray and black.\nblack color - active objects, those objects can be visited from root objects\ngray color - active objects, a temporary state which will later be marked as black\nwhite color - nonactive objects, the objects will later be free up\n  The algorithms is quite straight forward as belows\nmarkAllRootObjetcsToGray() for len(grayObjetcs) != 0: MarkOneGrayObjectsToBlack() MarkAllObjectsPointedByThatBlackedObjectToGray()   3. Write Barrier As we mentions above, STW(stop the world) can lead to a bad performance. Write barrier is introduced to solved the problem, it ensures the invariant in tri-color marking. Hence, we can restart the goroutines after the write barrier is turned on.\n1. Dijstra Write Barrier\nwritePointer(slot, ptr): shade(ptr) *slot = ptr   2. Yuasa Write Barrier\nwritePointer(slot, ptr) shade(*slot) *slot = ptr    Phase of Garbage Collection There are 3 phases in Go garbage collection.\n1. Mark Setup\n2. Marking\n3. Mark Termination\nLet\u0026rsquo;s go through the phases one by one\nMark Setup When a collection starts, the first activity that must be performed is turning on the Write Barrier. The purpose of the Write Barrier is to allow the collector to maintain data integrity on the heap during a collection since both the collector and application goroutines will be running concurrently.\nIn order to turn the Write Barrier on, every application goroutine running must be stopped. This activity is usually very quick, within 10 to 30 microseconds on average. That is, as long as the application goroutines are behaving properly.   Marking Once the Write Barrier is turned on, the collector commences with the Marking phase. The first thing the collector does is take 25% of the available CPU capacity for itself. The collector uses Goroutines to do the collection work and needs the same P’s and M’s the application Goroutines use. This means for our 4 threaded Go program, one entire P will be dedicated to collection work.\nIf the collector determines that it needs to slow down allocations, it will recruit the application Goroutines to assist with the Marking work. This is called a Mark Assist. The amount of time any application Goroutine will be placed in a Mark Assist is proportional to the amount of data it’s adding to heap memory. One positive side effect of Mark Assist is that it helps to finish the collection faster.   Mark Termination Once the Marking work is done, the next phase is Mark Termination. This is when the Write Barrier is turned off(STW again), various clean up tasks are performed, and the next collection goal is calculated. Goroutines that find themselves in a tight loop during the Marking phase can also cause Mark Termination STW latencies to be extended.\n Conclusion In this article, we only focus on the idea of Go garbage collection without actual implementation details. We can see that it is not free to allocate memory in Go, garbage collector takes time to do the memory release for us(although a programmer might not aware of it). As a programmer, we should use the allocation cleverly and put your faith and trust in the garbage collector to keep the heap healthy and your application running consistently.\n","permalink":"https://tsungjenh.github.io/blog/gc_go/","tags":["Go","Garbage Collection"],"title":"Garbage Collection in Go"},{"categories":["mysql"],"contents":" Intro In high concurrency environments, the database write operation will easily suffer due to data inconsistency issue. A simple way to resolve the issue is to use an exclusive lock. In this article, we are going to talk about the usage of SELECT ... FOR UPDATE statement which is an exlusive lock in MySQL InnoDB.\n A Common Race Condition Issue Suppose we have a table employee with the following schema.\nmysql\u0026gt; desc employee; +--------+---------------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +--------+---------------------+------+-----+---------+----------------+ | id | bigint(20) unsigned | NO | PRI | NULL | auto_increment | | name | varchar(32) | YES | MUL | NULL | | | salary | int(8) | NO | | NULL | | +--------+---------------------+------+-----+---------+----------------+ 3 rows in set (0.00 sec) mysql\u0026gt; SELECT * From employee; +----+-------+--------+ | id | name | salary | +----+-------+--------+ | 1 | Alex | 1300 | | 2 | Amy | 3000 | | 3 | Bruce | 5500 | | 4 | Jason | NULL | +----+-------+--------+ 4 rows in set (0.00 sec) There are two transaction,\n// Trasaction A to increase salary mysql\u0026gt;\u0026gt; set autocommit = 0; mysql\u0026gt;\u0026gt; SELECT * From employee WHERE id = 1; // a increase logic for salary mysql\u0026gt;\u0026gt; UPDATE employee SET salary = new_salary WHERE id=1; mysql\u0026gt;\u0026gt; commit; // Trasaction B to decrease salary mysql\u0026gt;\u0026gt; set autocommit = 0; mysql\u0026gt;\u0026gt; SELECT * From employee WHERE id = 1; // another calculation logic for salary mysql\u0026gt;\u0026gt; UPDATE employee SET salary = new_salary_2 WHERE id=1; mysql\u0026gt;\u0026gt; commit; If transaction B execute the SELECT statement before Transaction A execute the UPDATE statement. The data read by Transaction B will still be the old data which is the salary without increased. To prevent the issue happened, we want to a lock to prevent someone read the data while the write Operation are still going.\n SELECT \u0026hellip; FOR UPDATE Statement A SELECT \u0026hellip; FOR UPDATE reads the latest available data, setting exclusive locks on each row it reads. Thus, it sets the same locks a searched SQL UPDATE would set on the rows. If a row is locked by a transaction, a SELECT \u0026hellip; FOR UPDATE transaction that requests the same locked row must wait until the blocking transaction releases the row lock. This behavior prevents transactions from updating or deleting rows that are queried for updates by other transactions. However, waiting for a row lock to be released is not necessary if you want the query to return immediately when a requested row is locked, or if excluding locked rows from the result set is acceptable.\n The lock can be a row-level lock or a table-level lock depends on the query.\n  Row-level v.s. Table-level According to the Mysql InnoDB document, the conclusion is as belows\nCase 1: row is not existed -\u0026gt; No lock Case 2: query with index and hit the index -\u0026gt; Row is locked Case 3: query with index but index isn't hit -\u0026gt; Table is locked Case 4: query without index -\u0026gt; Table is locked  Row is not existed:  SELECT * FROM employee WHERE id=-1 FOR UPDATE; Query with index and hit the index:  # statement 1 mysql\u0026gt;\u0026gt; SELECT * From employee WHERE id = 1 FOR UPDATE; +----+------+--------+ | id | name | salary | +----+------+--------+ | 1 | Alex | 1300 | +----+------+--------+ 1 row in set (0.00 sec) // wait for a while to release mysql\u0026gt;\u0026gt; commit; ------------------------------------------------------------ // row is blocked mysql\u0026gt;\u0026gt; SELECT * From employee WHERE id = 1 FOR UPDATE; +----+------+--------+ | id | name | salary | +----+------+--------+ | 1 | Alex | 1300 | +----+------+--------+ 1 row in set (6.04 sec) ------------------------------------------------------------ // other row is not blocked mysql\u0026gt; SELECT * From employee WHERE id = 2 FOR UPDATE; +----+------+--------+ | id | name | salary | +----+------+--------+ | 2 | Amy | 3000 | +----+------+--------+ 1 row in set (0.00 sec) Query with index but index isn\u0026rsquo;t hit  // Query with index but index isn't hit mysql\u0026gt; SELECT * FROM employee WHERE salary is null FOR UPDATE; +----+-------+--------+ | id | name | salary | +----+-------+--------+ | 4 | Jason | NULL | +----+-------+--------+ 1 row in set (0.00 sec) // wait for a while to release mysql\u0026gt;\u0026gt; commit; ------------------------------------------------------------ // row is blocked mysql\u0026gt; SELECT * From employee WHERE id = 4 FOR UPDATE; +----+-------+--------+ | id | name | salary | +----+-------+--------+ | 4 | Jason | NULL | +----+-------+--------+ 1 row in set (4.30 sec) ------------------------------------------------------------ // other row is blocked mysql\u0026gt; SELECT * From employee WHERE id = 2 FOR UPDATE; +----+------+--------+ | id | name | salary | +----+------+--------+ | 2 | Amy | 3000 | +----+------+--------+ 1 row in set (3.48 sec) Query without index  // Query without index mysql\u0026gt; SELECT * From employee WHERE salary=1300 FOR UPDATE; +----+------+--------+ | id | name | salary | +----+------+--------+ | 1 | Alex | 1300 | +----+------+--------+ 1 row in set (0.00 sec) // wait for a while to release mysql\u0026gt;\u0026gt; commit; ------------------------------------------------------------ // row is blocked mysql\u0026gt; SELECT * From employee WHERE id = 1 FOR UPDATE; +----+------+--------+ | id | name | salary | +----+------+--------+ | 1 | Alex | 1300 | +----+------+--------+ 1 row in set (5.20 sec) ------------------------------------------------------------ // other row is blocked mysql\u0026gt; SELECT * From employee WHERE id = 2 FOR UPDATE; +----+------+--------+ | id | name | salary | +----+------+--------+ | 2 | Amy | 3000 | +----+------+--------+ 1 row in set (5.63 sec) ","permalink":"https://tsungjenh.github.io/blog/row_lock/","tags":["mysql"],"title":"Locking Rows in InnoDB: SELECT ... FOR UPDATE STATEMENT"},{"categories":["Go"],"contents":"0. Intro With its concurrency properties, Go becomes one of the most popular languages in recent years. In Go concurrency, we have this convenient data structure to communicate between multiple goroutines - Channels. In this article, we are going to dive into the source code and get a deep understanding of it.\n soucre code: src/runtime/chan.go  1. Data Structure First, take a look at the data structure.\ntype hchan struct { qcount uint // total data in the queue \tdataqsiz uint // size of the circular queue \tbuf unsafe.Pointer // points to an array of dataqsiz elements \telemsize uint16 closed uint32 elemtype *_type // element type \tsendx uint // send index \trecvx uint // receive index \trecvq waitq // list of recv waiters \tsendq waitq // list of send waiters  // lock protects all fields in hchan, as well as several \t// fields in sudogs blocked on this channel. \t// \t// Do not change another G\u0026#39;s status while holding this lock \t// (in particular, do not ready a G), as this can deadlock \t// with stack shrinking. \tlock mutex } // and bidirectional linked list type waitq struct { first *sudog last *sudog } // a wrapper of goroutine type sudog struct { // The following fields are protected by the hchan.lock of the \t// channel this sudog is blocking on. shrinkstack depends on \t// this for sudogs involved in channel ops.  g *g next *sudog prev *sudog elem unsafe.Pointer // data element (may point to stack)  // The following fields are never accessed concurrently. \t// For channels, waitlink is only accessed by g. \t// For semaphores, all fields (including the ones above) \t// are only accessed when holding a semaRoot lock.  acquiretime int64 releasetime int64 ticket uint32 // isSelect indicates g is participating in a select, so \t// g.selectDone must be CAS\u0026#39;d to win the wake-up race. \tisSelect bool parent *sudog // semaRoot binary tree \twaitlink *sudog // g.waiting list or semaRoot \twaittail *sudog // semaRoot \tc *hchan // channel } We can separate the channel into three parts, including a ring buffer, two wait queues and a lock.\n Ring buffer(buf) is a space (for the buffer channels) to store the temporary data (for a non-buffered channel, buf points to nil). The sendq and the recvq are two bidirectional linked list of send/recv waiter. The nodes in the waitq are sudog which is a wrapper of goroutines. lock is used to assert mutual exclusive to prevent race condition.  A visualization of the Channel data structure is as below.\n  2. Operations There are only 4 Operations for Channel.\n i. Create a channel ii. Send data to a channel iii. Receive data from a channel iv. Close a channel We will go through all the operations one by one.\ni. Create a channel To create a channel, go compiler translate make state to runtime.makechan\n make(chan interface{}, size) —\u0026gt; runtime.makechan(interface{}, size)\nmake(chan interface{}) —\u0026gt; runtime.makechan(interface{}, 0)\n Create a non-buffer channel is like creating a zero size channel. Take a look at the source code. The process breaks into 3 parts.\n Validate parameters(check if memory size is overflow). Create a ring buffer if size is greater than zero.(buffer channel) Assign value and create a lock  func makechan(t *chantype, size int) *hchan { elem := t.elem // compiler checks this but be safe.  if elem.size \u0026gt;= 1\u0026lt;\u0026lt;16 { throw(\u0026#34;makechan: invalid channel element type\u0026#34;) } if hchanSize%maxAlign != 0 || elem.align \u0026gt; maxAlign { throw(\u0026#34;makechan: bad alignment\u0026#34;) } mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u0026gt; maxAlloc-hchanSize || size \u0026lt; 0 { panic(plainError(\u0026#34;makechan: size out of range\u0026#34;)) } // create channel and allocate ring buffer for buffer channel \tvar c *hchan switch { case mem == 0: // Queue or element size is zero. \tc = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. \tc.buf = c.raceaddr() case elem.ptrdata == 0: // Elements do not contain pointers. \t// Allocate hchan and buf in one call. \tc = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // Elements contain pointers. \tc = new(hchan) c.buf = mallocgc(mem, elem, true) } // assign value and create lock \tc.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(\u0026amp;c.lock, lockRankHchan) if debugChan { print(\u0026#34;makechan: chan=\u0026#34;, c, \u0026#34;; elemsize=\u0026#34;, elem.size, \u0026#34;; dataqsiz=\u0026#34;, size, \u0026#34;\\n\u0026#34;) } return c } ii. Send/Receive data to/from channel Send data to channel First of all, a goroutine will always check if the channel is nil or blocked, if the channel is a nil channel. It will call gopark and will be blocked.forever. If a channel is closed, a panic will occur.\nfunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { if c == nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\u0026#34;unreachable\u0026#34;) } // ....  lock(\u0026amp;c.lock) // noted that always need to lock the channel to access or update channel data  if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;send on closed channel\u0026#34;)) } // ... }  Noted: If a channel is not nil, a sender needs to acquire the lock before any r/w to the channel.\n sendq and recv are two wait queues that consist of go routines. If a channel has no available space for buffering (a full channel or a non buffer channel) and a sender comes, the go routine will call gopark and will be blocked until a receiver comes. On the other hand, if a channel has no buffer data (the channel is empty or it is a non buffer channel), a receiver will call gopark and will be blocked until a sender comes.\nIn this case, a sender will always try to dequeue the recvq too see if there is a receiver. If there is a receiver in sendq, the sender will copy the data from it and goready the receiver.\nif sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send \t// directly to the receiver, bypassing the channel buffer (if any). \tsend(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true } Second, a sender will check if there is available space for buffering.\nif c.qcount \u0026lt; c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. \tqp := chanbuf(c, c.sendx) if raceenabled { raceacquire(qp) racerelease(qp) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(\u0026amp;c.lock) return true } If there no enough space for buffering (a full buffer channel or a non buffer channel) a sender will call gopark and will be blocked until a receiver comes.\n// Block on the channel. Some receiver will complete our operation for us. \tgp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg \t// on gp.waiting where copystack can find it. \tmysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) Receive data from channel First, a receiver will always check if the channel is nil. If it is a nil channel, a receiver will be blocked forever. However, it\u0026rsquo;s a bit different in the case of a closed channel. A receiver can always read the data from a closed channel (even if it is empty) and it will continue to loop the buffer and return the value of the pointer. This is why a receiver will always receive zero value from a closed empty channel\nfunc chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { // raceenabled: don\u0026#39;t need to check ep, as it is always on the stack \t// or is new memory allocated by reflect.  if debugChan { print(\u0026#34;chanrecv: chan=\u0026#34;, c, \u0026#34;\\n\u0026#34;) } if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(\u0026#34;unreachable\u0026#34;) } // ...  lock(\u0026amp;c.lock) if c.closed != 0 \u0026amp;\u0026amp; c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(\u0026amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } The following process is quite similar to the process of a sender. A receiver will always try to dequeue the sendq and check if there is any sender in the queue. If there is a sender in sendq, the receiver will copy the data from it and goready the sender\nif sg := c.sendq.dequeue(); sg != nil { // Found a waiting sender. If buffer is size 0, receive value \t// directly from sender. Otherwise, receive from head of queue \t// and add sender\u0026#39;s value to the tail of the queue (both map to \t// the same buffer slot because the queue is full). \trecv(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true, true } Second, check if there is data in the buffer. If yes, copy the data and return it\nif c.qcount \u0026gt; 0 { // Receive directly from queue \tqp := chanbuf(c, c.recvx) if raceenabled { raceacquire(qp) racerelease(qp) } if ep != nil { typedmemmove(c.elemtype, ep, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(\u0026amp;c.lock) return true, true } If there is no data in the buffer (a non buffer channel or an empty buffer channel) a receiver will call gopark and wait for a sender comes.\n// no sender available: block on this channel. \tgp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg \t// on gp.waiting where copystack can find it. \tmysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2)  To summarize the Send/Receive process,\n1. To send/recv to a nil channel, a go routine will be blocked forever.\n2. A send/recv need to acquire the lock before any r/w operations.\n3. To send to a closed channel, sender will panic, and a receiver will always get a data even if the channel is empty.\n4. If a buffer is full or nil, a sender will be blocked. If a buffer is empty or nil, receiver will be blocked.\n5. At most one of the queues (sendq/recvq) will be empty.\n iii. Close channel First, to close a channel, a go routine will check if the channel is nil or closed. A panic will occur in both cases.\n Noted that a lock is required to read the information on the channel.\n func closechan(c *hchan) { if c == nil { panic(plainError(\u0026#34;close of nil channel\u0026#34;)) } lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;close of closed channel\u0026#34;)) } //... } Second, there will be a temporary data structure, gList, to store the remaining node in the sendq/recvq and release the lock quickly.\n// release all readers \tfor { sg := c.recvq.dequeue() if sg == nil { break } if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = nil if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } // release all writers (they will panic) \tfor { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = nil if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } unlock(\u0026amp;c.lock) Once the lock is released. We can trigger goready for each sudog in the gList (temporary storage for releasing the lock quickly) as the closed channel is ready to be read by receivers.\n// Ready all Gs now that we\u0026#39;ve dropped the channel lock. \tfor !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } Conclusion Below is the comparison of the operations\n    Nil Channel Closed Channel Channel     Close Panic Panic Success   Send Blocked forever Panic Success or blocked   Receive Blocked forever always success success or blocked    ","permalink":"https://tsungjenh.github.io/blog/channel_ds/","tags":["Go","channel"],"title":"Go Channel Source Code Review"},{"categories":["map"],"contents":" 1. Definition: A hash table is a data structure that provides a mapping from keys to a values using techique called \u0026ldquo;hash\u0026rdquo;.\n2. Hash Function: A Hash function H(x) is a function that maps a key \u0026ldquo;x\u0026rdquo; to a hash table index in a fixed range.\n3. Properties of Hash Function  If H(x) = H(y) then value of x and y might be equals, but if H(x) != H(y) then value of x and y are certainly not equal. A hash function H(x) must be deterministic. This means that if H(x) = y then H(x) must always produce y and never produce other value. We will try very hard to make uniform hash function(ideally) to minimize the number of hash collision.  4. Hashable Since we are going to use hash functions in the implementation of our hash table, we need our hash functions to be deterministic. To enforce this behavior we demand that the keys used in our hash table to be immutable data types. Hence, if a key of type-T is immutable, and we have a hash function H(k) defined for all keys k of type-T then we say a key of type-T is hashable.\n In Python, mutable type such as \u0026ldquo;list\u0026rdquo;, \u0026ldquo;set\u0026rdquo;, \u0026ldquo;dict\u0026rdquo; etc.. are not hashable.\n 5. How a Hash Table Work? Ideally we would like to have a very fast insertion, lookup and removal time for the data. we are placing in our hash table. Remarkably, we can achieve all this in O(1) time using a hash function as a way to index into hash tables.\n The constant time behavior attributed to hash tables is only true if you have a good uniform hash function\n 6. What is Collision and How to Handle? A hash collision is when two key x, y hash to the same value which is H(x) = H(y) There are 2 primary ways to handle this situation:\n1. Separate chaining:\nThis approach deals with hash collisions by maintaining a data structure (usaually a linked list) to hold all the different values which hashed to a particular value.\n2. Open Addressing:\nThis approach deals with hash collisions by finding another place within the hash table for the k-v to go by offsetting(by probing function P(x)) it from the position to which it hashed to.\n7. Chaos with cycles If we implement hash table with open addressing and collision occurs, we have to use the probing function to get the next position to store the value. There might be a issue of causing cycle. (there are still available position in hash table, but probing can\u0026rsquo;t reach due to the cycle).\nTo solve this problem we can do it by properly set the size and probing factor in different kind of probing strategies. Rules:\n1. linear - ax + b \u0026ndash;\u0026gt; let a = 1 or gcd(a, size) = 1\n2. quadratic - ax^2 + bx + c \u0026ndash;\u0026gt; choose (x^2 + x) / 2 and size is a power of 2\ndouble hashing - p(x, k) = x * h\u0026rsquo;(x) \u0026ndash;\u0026gt; linear hashing \u0026ndash;\u0026gt; gcd(h\u0026rsquo;(x), size) must be 1 \u0026ndash;\u0026gt; choose size to be prime.\n8. Max Load Factor A hash table has to adjust the size as the data stored increases. A max load factor is a flag to indicated whether the table size should increase. A proper used of max load factor can prevent the worst case complexity occurs.\n9. Complexity    Operation Average Worst     Insertion O(1) O(N)   Removal O(1) O(N)   Search O(1) O(N)     In the worst case which every key is hashed to the same hash idx, hash table depreciates to a linear structure such as array or linked list.\n 10. Implementation  Separate Chaining Hash Table:  class Node: def __init__(self, key, x): self.key = key self.data = x self.next = None class HashWithSeparateChaining: def __init__(self, size): if size \u0026lt;= 0: raise Exception(\u0026#39;size must greater than 0\u0026#39;) self.hash_table = [None for _ in range(size)] self.count = 0 self.size = size self.max_load_factor = 0.4 def _get_threshold(self): return int(self.size * self.max_load_factor) def __repr__(self): ret = \u0026#39;\u0026#39; for idx, e in enumerate(self.hash_table): line = \u0026#39;idx: {}, chaining: \u0026#39;.format(idx) chaining = [] while e is not None: chaining.append(\u0026#39;(key:{}, value:{})\u0026#39;.format(e.key, e.data)) e = e.next ret += line + \u0026#39;-\u0026gt;\u0026#39;.join(chaining) + \u0026#39;\\n\u0026#39; return ret def _hash_func(self, key): return key % self.size def _expand_and_reallocate(self): self.size = self.size * 2 pre_hash_table = self.hash_table self.hash_table = [None for _ in range(self.size)] for e in pre_hash_table: while e is not None: self.insert(e.key, e.data, is_relocated=True) e = e.next def insert(self, key, value, is_relocated=False): hash_idx = self._hash_func(key) idx_head = self.hash_table[hash_idx] if idx_head is None: self.hash_table[hash_idx] = Node(key, value) else: while idx_head.next is not None: if idx_head.key == key and idx_head.data == value: return idx_head = idx.next idx_head.next = Node(key, value) self.count += 1 if self.count \u0026gt; self._get_threshold() and not is_relocated: self._expand_and_reallocate() def search(self, key): hash_idx = self._hash_func(key) idx_head = self.hash_table[hash_idx] while idx_head is not None: if idx_head.key == key: return idx_head.data idx_head = idx_head.next raise Exception(\u0026#39;key not in found\u0026#39;) def remove(self, key): hash_idx = self._hash_func(key) idx_head, pre = self.hash_table[hash_idx], None while idx_head is not None: if idx_head.key == key: if pre is None: self.hash_table[hash_idx] = idx_head.next else: pre.next = idx_head.next self.count -= 1 return pre = idx_head idx_head = idx_head.next raise Exception(\u0026#39;key not in found\u0026#39;) Open Addressing Hash Table:  class Node: def __init__(self, key, x): self.key = key self.data = x class DeadNode: pass class OpenAddressingHashTable: def __init__(self, size=10): if size \u0026lt;= 1: raise Exception(\u0026#39;hash table size less than 1\u0026#39;) self.hash_table = [None for _ in range(size)] self.size = size self.total_count = 0 self.max_load_factor = 0.4 def __repr__(self): ret = \u0026#39;\u0026#39; for idx, e in enumerate(self.hash_table): line = \u0026#39;idx: {}, \u0026#39;.format(idx) if e is not None: if e is DeadNode: line += \u0026#39;Deadnode\u0026#39; else: line += \u0026#39;key: {}, val: {}\u0026#39;.format(e.key, e.data) else: line += \u0026#39;None\u0026#39; ret += line + \u0026#39;\\n\u0026#39; return ret def _get_threshold(self): return int(self.size * self.max_load_factor) def _hash_func(self, key): return key % self.size def _probing_func(self, hash_idx, count): offset = count + 0 return (hash_idx + offset) % self.size def _expand_and_reallocate(self): self.size = self.size * 2 pre_hash_table = self.hash_table self.hash_table = [None for _ in range(self.size)] for e in hash: if e is not None: self.insert(e.key, e.data) def _insert(self, idx, node): self.hash_table[idx] = node self.total_count += 1 if self.total_count \u0026gt; self._get_threshold(): self._expand_and_reallocate() return def insert(self, key, value): hash_idx = self._hash_func(key) idx = hash_idx prob_term = 1 idx_set = set() while 1: idx_set.add(idx) node = self.hash_table[idx] if node is None or node is DeadNode: self._insert(idx, Node(key, value)) return elif node.key == key: return idx = self._probing_func(hash_idx, prob_term) prob_term += 1 if idx in idx_set: # chaos of cycles if len(prob_idx_set) == self.size: self._expand_and_reallocate() self.insert(key, value) break else: raise Exception(\u0026#39;hash table issue: chaos of cycles\u0026#39;) def search(self, key, to_return_idx=False): hash_idx = self._hash_func(key) idx = hash_idx prob_term = 1 idx_set = set() while 1: idx_set.add(idx) node = self.hash_table[idx] if node is None: raise Exception(\u0026#39;key not found\u0026#39;) elif node is not DeadNode: if node.key == key: return node.data if not to_return_idx else idx idx = self._probing_func(hash_idx, prob_term) prob_term += 1 if idx in idx_set: if len(idx_set) == self.size: raise Exception(\u0026#39;key not found\u0026#39;) else: raise Exception(\u0026#39;hash table issue: chaos of cycles\u0026#39;) def remove(self, key): idx = self.search(key, to_return_idx=True) self.hash_table[idx] = DeadNode ","permalink":"https://tsungjenh.github.io/blog/hashmap/","tags":["map","python"],"title":"Implementation of Hash Map in Python"}]